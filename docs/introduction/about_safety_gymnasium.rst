简介
====


关于Safe RL
-----------

近年来，RL（强化学习）算法，特别是DeepRL算法在许多任务中都取得了良好的性能。比如：在Atari游戏中仅用视觉输入就获得高分，在高维度上完成复杂的控制任务，以及在围棋比赛中击败人类大师。然而，在RL的策略更新过程中，Agent **经常** 学习到 **reward hacking** 甚至 **危险行为** 以 **提高其累积reward**。这样可能 **导致片面追求reward** 而 **不能满足我们设计reward的初衷**。因此，**Safe RL算法致力于** 训练Agent在 **最大化reward的同时满足给定的约束条件**，从而避免学习到一些脱离现实而片面追求reward的行为策略。

强化学习可以理解为Agent通过给定的reward信号学习，不断优化自身策略，在解决无法严格进行数学建模的问题时十分有效。在此基础上，Safe RL也可以广义地理解为一个**约束求解问题**：Agent需要在不断学习reward信号的同时对约束(cost)信号进行学习，从而对难以有效建模的约束进行学习，在满足约束的前提下，最大化reward。

Safety-Gymnasium
----------------

Safety-Gymnasium是为了服务于Safe RL领域研究而基于Python开发的一个 **高度模块化**，**代码精简易读** 并且 **易于自定义** 的基准环境。

Feature
^^^^^^^^

- 高度模块化，将一个环境的不同组成部分拆分为逻辑自洽的不同模块：agent, task, objects。
- 代码精简易读，在保证 **可读性** 和 **美观性** 的前提下最大程度精简代码，每一个任务平均 **不超过100行** 代码。
- 易于自定义，**精心设计的代码框架** 对于自定义环境的需求十分友好。
- 丰富可靠的环境，提供了 **Manipualtion** 和 **Vision** 两个新类别的任务。
- 对经典Safe RL环境的良好支持与重构： **Safety-Gym** ，**Safety-Velocity**。
- 依赖少。
- 易于安装。

